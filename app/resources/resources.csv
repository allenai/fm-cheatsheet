Type,Modalities,Name,Description,Release MM-YY,Type,Paper Link,Website Link,GitHub Link,HF Link,Additional Link,Additional Link Info,Added By,
Code Reproducibility,All,Anaconda,An environment and dependency management tool.,Frequently Updated,Webpage,,https://www.anaconda.com/,,,,,Shayne,
Code Reproducibility,All,Docker,An environment and dependency management tool.,Frequently Updated,Webpage,,https://docker-curriculum.com/,,,,,Shayne,
Code Reproducibility,All,Colab Notebooks,A tool to execute and share reproducible code snippets.,Frequently Updated,Webpage,,https://colab.research.google.com/,,,,,Shayne,
Code Reproducibility,All,Jupyter Notebooks,A tool to execute and share reproducible code snippets.,Frequently Updated,Webpage,,https://jupyter.org/,,,,,Shayne,
Code Reproducibility,All,Semver,"A widely used protcol for versioning to software, to ensure easy reproducibility.",,Webpage,,https://semver.org/,,,,,Shayne,
Code Reproducibility,Text,OpenLM,"OpenLM is a minimal language modeling repository, aimed to facilitate research on medium sized LMs. They have verified the performance of OpenLM up to 7B parameters and 256 GPUs. They only depend only on PyTorch, XFormers, or Triton.",Frequently Updated,GitHub,,,https://github.com/mlfoundations/open_lm,,,,Shayne,
Code Reproducibility,Text,LM Evaluation Harness,"Orchestration framework for standardizing LM prompted evaluation, supporting hundreds of subtasks.",Frequently Updated,GitHub,,,https://github.com/EleutherAI/lm-evaluation-harness,,,,Stella,
Data Auditing,Text,Quality at a Glance,An audit of allegedly multilingual parallel text corpora.,3-2021,Paper,https://arxiv.org/abs/2103.12028,,,,,,Stella,
Data Auditing,Text,"Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus",A third party datasheet for BookCorpus,5-2021,Paper,https://arxiv.org/abs/2105.05241,,,,,,Stella,
Data Auditing,Text,Datasheet for the Pile,A datasheet for the Pile,1-2022,Paper,https://arxiv.org/abs/2201.07311,,,,,,Stella,
Data Auditing,Text,Data Provenance Initiative,A large scale audit of 2000+ popular datasets in AI.,Frequently Updated,Paper,https://arxiv.org/abs/2310.16787,https://www.dataprovenance.org/,https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection,https://huggingface.co/DataProvenanceInitiative,,,Stella,
Data Auditing,Text+Vision,HaveIBeenTrained,A combination search tool / opt out tool for LAION,Frequently Updated,Webpage,,https://haveibeentrained.com/,,,,,Stella,
Data Auditing,All,Training Data Transparency Blog,A blog on transparency for training data in AI.,12-2023,Webpage,,https://huggingface.co/blog/yjernite/data-transparency,,,,,Shayne,
Data Auditing,Text+Vision,"Multimodal datasets: misogyny, pornography, and malignant stereotypes",Auditing vision datasets for sensitive content.,10-2021,Paper,https://arxiv.org/abs/2110.01963,,,,,,Shayne / Stella,
Data Auditing,Text+Vision,On Hate Scaling Laws For Data-Swamps,Auditing text and vision datasets for systemic biases and hate.,6-2023,Paper,https://arxiv.org/abs/2306.13141,,,,,,Shayne / Stella,
Data Auditing,Text+Vision,Into the LAIONs Den: Investigating Hate in Multimodal Datasets,Auditing hateful content in text-to-vision datasets.,9-2023,Paper,https://arxiv.org/abs/2311.03449,,,,,,Shayne / Stella,
Data Cleaning & Filtering,Text,Dolma's Toolkit,"A Python framework for defining Taggers that identify non-language text, language ID, PII, toxic text, and ""quality"" text. Includes reimplementation of heuristics used by Gopher and C4 for non-natural language.",8-2023,Hugging Face object,,,https://github.com/allenai/dolma,,,,Kyle,
Data Cleaning & Filtering,Text+Vision,DataComp pre-filtering,"NSFW detection, dedup with eval datasets",4-2023,Webpage,https://arxiv.org/abs/2304.14108,https://www.datacomp.ai/,https://github.com/mlfoundations/dataset2metadata,https://huggingface.co/datasets/mlfoundations/datacomp_1b,,,Gabriel,
Data Cleaning & Filtering,Text+Vision,DataComp filtering,Various quality filters,4-2023,Webpage,https://arxiv.org/abs/2304.14108,https://www.datacomp.ai/,https://github.com/mlfoundations/datacomp/tree/main#baselines,https://huggingface.co/datasets/mlfoundations/datacomp_1b,,,Gabriel,
Data Cleaning & Filtering,Speech,SpeechBrain’s Spoken language ID model,"Pre-trained spoken language identification model trained on VoxLingua107, dataset of audio sourced from YouTube for 107 languages",6-2021,Hugging Face object,https://arxiv.org/abs/2106.04624,,,https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa,,,Nay,
Data Cleaning & Filtering,Text,Data Selection via Importance Resampling (DSIR),A tool for selecting data with a similar distribution to a target dataset,12-2023,Paper,https://arxiv.org/abs/2302.03169,,https://github.com/p-lambda/dsir,,,,Alon,
Data Cleaning & Filtering,Text,Langdetect,"A tool to predict the language of text, used to filter out/in data from the desired languages",5-2021,GitHub,,https://pypi.org/project/langdetect/,https://github.com/Mimino666/langdetect,,,,Alon,
Data Cleaning & Filtering,Text,fastText language classifier,A tool for classifying the language of text,5-2023,Hugging Face object,,,,https://huggingface.co/facebook/fasttext-language-identification,,,Alon,
Data Cleaning & Filtering,Text,Lilac,"A python package for better understanding your data. Includes keyword and semantic search, as well as detection for PII, duplicates, and language.",9-2023,GitHub,,https://www.lilacml.com/,https://github.com/lilacai/lilac,,,,Alon,
Data Cleaning & Filtering,Text,Roots data cleaning pipeline,A pipeline for processing and improving quality of crowdsourced datasets,10-2022,GitHub,,,https://github.com/bigscience-workshop/data-preparation/tree/main/preprocessing/training/01a_catalogue_cleaning_and_filtering,,,,Alon,
Data Cleaning & Filtering,Text,The Pile processing scripts,"A series of scripts to replicate the Pile dataset. Includes filtering and cleaning for: language, profanity, deduplication, and test set decontamination.",12-2020,GitHub,,,https://github.com/EleutherAI/the-pile/tree/master/processing_scripts,,,,Alon,
Data Cleaning & Filtering,Text,FUN-LangID,"Frequently Used N-grams Language ID model, a character 4-gram model trained to recognize up to 1633 languages.",9-2023,GitHub,,,https://github.com/google-research/url-nlp/tree/main/fun-langid,,,,Alon,
Data Cleaning & Filtering,Text,OpenLID,A model (and data used to train the model) for identifying 200+ languages.,,GitHub,https://arxiv.org/abs/2305.13820,,https://github.com/laurieburchell/open-lid-dataset,,,,Alon,
Data Cleaning & Filtering,Text,GlotLID,"A model for identifying languages, with support for more than 1600 languages.",,GitHub,https://arxiv.org/abs/2310.16248,,https://github.com/cisnlp/GlotLID,,,,Alon,
Data Cleaning & Filtering,Text,Detoxify,"A python library designed to identify toxic language in comments. Functions in seven languages: English, Italian, French, Russian, Portuguese, Spanish, Turking.",,GitHub,,,https://github.com/unitaryai/detoxify,,,,Alon,
Data Decontamination,Text,BigBench Canaries,"BigBench's ""Training on the Test Set"" Task provies guidance on using canaries to check if an evaluation set was trained on.",10-2021,GitHub,,,https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/training_on_test_set/README.md#training-on-the-test-set,,,,Shayne,
Data Decontamination,Text,Carper AI Decontamination Tool,"A repository, heavily based by the BigCode repository, to decontaminate evaluation sets from a text training set.",1-2023,GitHub,,,https://github.com/CarperAI/decontamination/tree/main,,,,Shayne,
Data Decontamination,Text,Data Portraits,"A tool to test for membership inference of popular datasets, like The Pile or The Stack, i.e. whether a model has seen certain data.",3-2023,Webpage,https://arxiv.org/abs/2303.03919,https://dataportraits.org/,,,,,Shayne,
Data Decontamination,Text,Interpreting Canary Exposure,"An explanation on how to interpret canary exposure, including by relating it to membership inference attacks, and differential privacy.",5-2023,Paper,https://arxiv.org/abs/2306.00133,,,,,,Shayne,
Data Decontamination,Text,Proving Test Set Contamination in Black Box Language Models,A paper that provides methods for provable guarantees of test set contamination in language models without access to pretraining data or model weights.,10-2023,Paper,https://arxiv.org/abs/2310.17623,,,,,,Shayne,
Data Decontamination,Text,Detect Pretrain Data (Min-K Prob),"A codebase that implements ""Min-K% Prob"", a method that can detect if a large language model was pretrained on some text.",11-2023,Webpage,https://arxiv.org/abs/2310.16789,https://swj0419.github.io/detect-pretrain.github.io/,https://github.com/swj0419/detect-pretrain-code,,,,Shayne,
Data Decontamination,Text+Vision,HaveIBeenTrained,"A combination search tool / opt out tool for datasets, like LAION. Data creators can see if their works appear in common datasets.",Frequently Updated,Webpage,,https://haveibeentrained.com/,,,,,Stella,
Data Deduplication,Text,Google Text Deduplication,"A repository to deduplicate language model datasets. They release the ExactSubstr deduplication implementation (written in Rust) along with scripts to perform ExactSubstr deduplication and inspect the results (written in Python). They also release the document clusters resulting from running NearDup deduplication on C4, RealNews, LM1B, and Wiki-4B-en.",7-2021,GitHub,https://arxiv.org/abs/2107.06499,,https://github.com/google-research/deduplicate-text-datasets,,,,Shayne,
Data Deduplication,Text,RedPajama-Data,"Tools for: exact deduplication with bloom filter, fuzzy deduplication with LSH, calculating quality scores",10-2023,GitHub,,,https://github.com/togethercomputer/RedPajama-Data,,,,Alon,
Data Deduplication,Text,Dolma Dedupe Tool,Dolma's text deduplication tool for pretraining data,10-2023,GitHub,,,https://github.com/allenai/dolma,,,,,
Data Deduplication,Text,EleutherAI's deduplication code,EleutherAI's text deduplication tool for pretraining data,5-2023,GitHub,,,https://github.com/EleutherAI/pile_dedupe,,,,,
Data Deduplication,Vision,Datacomp image dedup,Data to deduplicate vision datasets for the Datacomp challenge.,8-2023,GitHub,,https://www.datacomp.ai/,https://github.com/mlfoundations/dataset2metadata,,,,,
Data Deduplication,All,Apricot,apricot implements submodular optimization for the purpose of summarizing massive data sets into minimally redundant subsets that are still representative of the original data. These subsets are useful for both visualizing the modalities in the data (such as in the two data sets below) and for training accurate machine learning models with just a fraction of the examples and compute.,2020,GitHub,https://jmlr.org/papers/volume21/19-467/19-467.pdf,,https://github.com/jmschrei/apricot,,,,,
Data Governance,Text+Vision,HaveIBeenTrained,A combination search tool / opt out tool for LAION,Frequently Updated,Webpage,,https://haveibeentrained.com/,,,,,Stella,
Data Governance,Text,Reclaiming the Digital Commons: A Public Data Trust for Training Data,A paper that argues for the creation of a public data trust for collective input into the creation of AI systems and analyzes the feasibility of such a data trust.,3-2023,Paper,https://arxiv.org/abs/2303.09001,,,,,,Sayash,
Data Governance,All,Data Governance in the Age of Large-Scale Data-Driven Language Technology,A paper detailing the data governance decisions undertaken during BigScience's BLOOM project. ,5-2022,Paper,https://dl.acm.org/doi/abs/10.1145/3531146.3534637,,,,,,Sayash,
"Dataset Search, Analysis, & Exploration",Text,Data Finder,A tool to help build search over academic datasets given a natural language description of the idea.,5-2023,GitHub,https://arxiv.org/abs/2305.16636,,https://github.com/viswavi/datafinder,,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,GAIA Search Tool,"A search tool over C4, the Pile, ROOTS, and the text captions of LAION, developed with Pyserini (https://github.com/castorini/pyserini).",6-2023,Hugging Face object,https://arxiv.org/abs/2306.01481,,,https://huggingface.co/spaces/spacerini/gaia,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,WIMBD,"A dataset analysis tool to count, search, and compare attributes across several massive pretraining corpora at scale, including C4, The Pile, and RedPajama.",11-2023,Webpage,https://arxiv.org/abs/2310.20707,https://wimbd.apps.allenai.org/,https://github.com/allenai/wimbd,,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,Data Provenance Explorer,"An explorer tool for selecting, filtering, and visualizing popular finetuning, instruction, and alignment training datasets from Hugging Face, based on their metadata such as source, license, languages, tasks, topics, among other properties.",Frequently Updated,Webpage,https://arxiv.org/abs/2310.16787,https://www.dataprovenance.org/,https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection,https://huggingface.co/DataProvenanceInitiative,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,AI2 C4 Search Tool,A search tool that lets users to execute full-text queries to search Google's C4 Dataset.,2021,Webpage,,https://c4-search.apps.allenai.org/,,,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,ROOTS Search Tool,"A tool, based on a BM25 index, to search over text for each language or group of languages included in the ROOTS pretraining dataset.",2021,Hugging Face object,,,,https://huggingface.co/spaces/bigscience-data/roots-search,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text,Hugging Face Data Measurements Tool,"A tool to analyze, measure, and compare properties of text finetuning data, including their distributional statistics, lengths, and vocabularies.",2021,Hugging Face object,,,,https://huggingface.co/spaces/huggingface/data-measurements-tool,,,Shayne,
"Dataset Search, Analysis, & Exploration",Text+Vision,LAION search,Nearest neighbor search based on CLIP embeddings,3-2022,Webpage,,https://rom1504.github.io/clip-retrieval/,https://github.com/rom1504/clip-retrieval,,,,Gabriel,
"Dataset Search, Analysis, & Exploration",Vision,Know your data,A tool for exploring over 70 vision datasets,5-2021,Webpage,,https://knowyourdata-tfds.withgoogle.com/,https://github.com/PAIR-code/knowyourdata,,,,Gabriel,
"Dataset Search, Analysis, & Exploration",Speech,NVIDIA Speech Data Explorer,Tool for exploring speech data,Frequently Updated,Webpage,,https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tools/speech_data_explorer.html,,,,,Nay,
Environmental Impact,All,CodeCarbon,"Estimate and track carbon emissions from your computer, quantify and analyze their impact.",11-2020,GitHub,,https://mlco2.github.io/codecarbon/,https://github.com/mlco2/codecarbon,,,,Stella & Aviya,
Environmental Impact,All,Experiment Impact Tracker,"The experiment-impact-tracker is meant to be a simple drop-in method to track energy usage, carbon emissions, and compute utilization of your system. Currently, on Linux systems with Intel chips (that support the RAPL or powergadget interfaces) and NVIDIA GPUs, we record: power draw from CPU and GPU, hardware information, python package versions, estimated carbon emissions information, etc. In California we even support realtime carbon emission information by querying caiso.com!",1-2020,GitHub,https://arxiv.org/abs/2002.05651,,https://github.com/Breakend/experiment-impact-tracker,,,,Stella & Aviya,
Environmental Impact,All,Carbontracker,carbontracker is a tool for tracking and predicting the energy consumption and carbon footprint of training deep learning models as described in Anthony et al. (2020).,7-2020,GitHub,https://arxiv.org/abs/2007.03051,,https://github.com/lfwa/carbontracker,,,,Stella & Aviya,
Environmental Impact,All,ML CO2 Impact,A tool for estimating carbon impacts of ML training,10-2019,Webpage,https://arxiv.org/abs/1910.09700,https://mlco2.github.io/impact/,,,,,Stella & Aviya,
Environmental Impact,All,Azure Emissions Impact Dashboard,Monitoring the environmental impact of training machine learning models on Azure,10-2021,Webpage,,https://www.microsoft.com/en-us/sustainability/emissions-impact-dashboard,,,,,Sayash,
Environmental Impact,All,Google Cloud Carbon Footprint Measurement,Tracking the emissions of using Google's cloud compute resources,10-2021,Webpage,,https://cloud.google.com/carbon-footprint?hl=en,,,,,Sayash,
Environmental Impact,Text,Training Compute-Optimal Large Language Models,Provides details on the optimal model size and number of tokens for training a transformer-based language model in a given computational budget.,3-2022,Paper,https://arxiv.org/abs/2203.15556,,,,,,Sayash,
Environmental Impact,Text,Scaling Laws for Neural Language Models,Provide scaling laws to determine the optimal allocation of a fixed compute budget.,1-2020,Paper,https://arxiv.org/abs/2001.08361,,,,,,Sayash,
Environmental Impact,Text,"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",A comprehensive account of the broader environmental impact of the BLOOM language model.,6-2023,Paper,https://jmlr.org/papers/v24/23-0069.html,,,,,,Sayash,
Finetuning Data,Text,Data Provenance Collection,"A repository and explorer tool for selecting popular finetuning, instruction, and alignment training datasets from Hugging Face, based on data provenance and characteristics criteria.",Frequently Updated,Webpage,https://arxiv.org/abs/2310.16787,https://www.dataprovenance.org/,https://github.com/Data-Provenance-Initiative/Data-Provenance-Collection,https://huggingface.co/DataProvenanceInitiative,,,Shayne,
Finetuning Data,Text+Speech,Lanfrica,An online catalogue that provides links to African language resources (papers and datasets) in both texts and speech,Frequently Updated,Webpage,,https://lanfrica.com/,,,,,David,
Finetuning Data,Text+Speech,Zenodo AfricaNLP Community,An online catalogue that provides African language resources (data and models) in both texts and speech,Frequently Updated,Webpage,,https://zenodo.org/communities/africanlp,,,,,David,
Finetuning Data,Text+Speech,Masakhane NLP,"A repository of African language text and speech resources, including datasets.",Frequently Updated,GitHub,,https://www.masakhane.io/,https://github.com/masakhane-io,https://huggingface.co/masakhane,,,Shayne,
Finetuning Data,Text+Speech,Indonesian NLP Data Catalogue,A respository of hundreds of Indonesian language datasets.,Frequently Updated,Webpage,,https://indonlp.github.io/nusa-catalogue/,https://github.com/IndoNLP/nusa-crowd,https://huggingface.co/NusaCrowd,,,Shayne,
Finetuning Data,Text+Speech,SEACrowd,A repository of hundreds of South East Asian language datasets.,Frequently Updated,Webpage,,https://seacrowd.github.io/seacrowd-catalogue/,https://github.com/SEACrowd,https://huggingface.co/NusaCrowd,,,Shayne,
Finetuning Data,Text+Speech,Arabic NLP Data Catalogue,"A catalogue of hundreds of Arabic text and speech finetuning datasets, regularly updated.",Frequently Updated,Webpage,,https://arbml.github.io/masader/,https://github.com/ARBML,,,,Shayne,
Finetuning Data,Text+Vision,MSCOCO,"Object detection, segemndation, captioning and retrieval dataset",5-2014,Webpage,https://arxiv.org/abs/1405.0312,https://cocodataset.org/#home,,,,,Gabriel,
Finetuning Data,Vision,ImageNet,An image classification dataset with 1.3M samples and 1000 classes,6-2009,Webpage,https://image-net.org/static_files/papers/imagenet_cvpr09.pdf,https://www.image-net.org/,,,,,Gabriel,
Finetuning Data,Speech,OpenSLR,A collection of user-contributed datasets for various speech processing tasks,Frequently Updated,Webpage,,https://www.openslr.org/resources.php,,,,,Nay,
Finetuning Data,Speech,VoxLingua107,Spoken language identification dataset created using audio extracted from YouTube videos retrieved using language-specific search phrases,11-2020,Webpage,https://arxiv.org/abs/2011.12998,https://bark.phon.ioc.ee/voxlingua107/,,https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa,,,Nay,
Finetuning Data,Speech,VoxCeleb,Speaker Identification dataset comprising of YouTube interviews from thousands of celebrities,6-2017,Webpage,https://arxiv.org/abs/1706.08612,https://www.robots.ox.ac.uk/~vgg/data/voxceleb/,,,,,Nay,
Finetuning Data,Speech,CHiME-5,Speaker Diarization dataset comprising over 50 hours of conversational speech recordings collected from twenty real dinner parties that have taken place in real homes,2018,Webpage,https://licensing.sheffield.ac.uk/product/chime5/print,https://licensing.sheffield.ac.uk/product/chime5,,,,,Nay,
Capabilities Evaluation,Text,LM Evaluation Harness,"Orchestration framework for standardizing LM prompted evaluation, supporting hundreds of subtasks.",Frequently Updated,GitHub,,https://github.com/EleutherAI/lm-evaluation-harness,https://github.com/EleutherAI/lm-evaluation-harness,,,,Hailey,
Capabilities Evaluation,Text+Vision,CLIP benchmark,"Image classification, retrieval and captioning",4-2022,GitHub,,,https://github.com/LAION-AI/CLIP_benchmark,,,,Gabriel,
Capabilities Evaluation,Text+Vision,DataComp eval suite,38 image classification and retrieval downstream tasks,4-2023,GitHub,https://arxiv.org/abs/2304.14108,https://www.datacomp.ai/,https://github.com/mlfoundations/datacomp#evaluation,,https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv,Leaderboard,Gabriel,
Capabilities Evaluation,Text+Vision,MME,multiple-choice QA,6-2023,GitHub,https://arxiv.org/abs/2306.13394,,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/,,,,Gabriel,
Capabilities Evaluation,Text+Vision,MMBench,multiple-choice QA,7-2023,GitHub,https://arxiv.org/abs/2307.06281,https://opencompass.org.cn/mmbench,https://github.com/open-compass/MMBench,,,,Gabriel,
Capabilities Evaluation,Text+Vision,OpenFlamingo eval suite,"VQA, captioning, classification",8-2023,GitHub,https://arxiv.org/abs/2308.01390,,https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/eval,,,,Gabriel,
Capabilities Evaluation,Speech,The Edinburgh International Accents of English Corpus,Benchmark dataset of diverse English varieties for evaluating automatic speech recognition models (typically trained and tested only on US English),3-2023,Webpage,https://arxiv.org/abs/2303.18110,https://groups.inf.ed.ac.uk/edacc/,,,,,Nay,
Capabilities Evaluation,Text,HELM,"A large suite of 100s of benchmarks and 100s of metric types, to holistically evaluate many model qualities aside from performance on general tasks. Useful for a thorough comparison against other well known models.",11-2022,Webpage,https://arxiv.org/abs/2211.09110,https://crfm.stanford.edu/helm/latest/,https://github.com/stanford-crfm/helm,,,,Shayne,
Capabilities Evaluation,Text,BigBench,"A collaborative benchmark of 100s of tasks, probing LLMs on a wide array of unique capabilities.",6-2022,GitHub,https://arxiv.org/abs/2206.04615,,https://github.com/google/BIG-bench,,,,Shayne,
Capabilities Evaluation,Text,BigBench Hard,A challenging subset of 23 BigBench tasks where at time of release models did not outperform annotator performance.,10-2022,GitHub,https://arxiv.org/abs/2210.09261,,https://github.com/suzgunmirac/BIG-Bench-Hard,,,,Shayne,
Capabilities Evaluation,Text+Vision,HEIM,A large suite of text-to-image evaluations. Useful for thorough capability analysis of these model types.,11-2023,Webpage,https://arxiv.org/abs/2311.04287,https://crfm.stanford.edu/heim/v1.1.0/,,,,,Shayne,
Capabilities Evaluation,Text,BigCode Evaluation Harness,"A framework for the evaluation of code generation models, compiling many evaluation sets.",Frequently Updated,GitHub,,,https://github.com/bigcode-project/bigcode-evaluation-harness/tree/main,,,,Shayne,
Capabilities Evaluation,Text,HumanEvalPack,"HumanEvalPack is a code evaluation benchmark across 6 languages and 3 tasks, extending OpenAI's HumanEval.",8-2023,GitHub,https://arxiv.org/abs/2308.07124,,https://github.com/bigcode-project/octopack,https://huggingface.co/datasets/bigcode/humanevalpack,,,Shayne,
Capabilities Evaluation,Text,SWE Bench,"SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem.",10-2023,Webpage,https://arxiv.org/abs/2310.06770,https://www.swebench.com/,https://github.com/princeton-nlp/SWE-bench,,,,Shayne,
Capabilities Evaluation,Text,MTEB,"The Massive Text Embedding Benchmark measures the quality of embeddings across 58 datasets and 112 languages for tasks related to retrieval, classification, clustering or semantic similarity.",10-2022,GitHub,https://arxiv.org/abs/2210.07316,,https://github.com/embeddings-benchmark/mteb,https://huggingface.co/spaces/mteb/leaderboard,,,Shayne,
Capabilities Evaluation,All,Hugging Face Leaderboards,A set of popular model leaderboards on Hugging Face for ranking on generic metrics.,Frequently Updated,Hugging Face object,,,,https://huggingface.co/open-llm-leaderboard,,,Shayne,
Capabilities Evaluation,Speech,OpenASR Leaderboard,An automatic leaderboard ranking and evaluating speech recognition models on common benchmarks.,Frequently Updated,Hugging Face object,,,https://github.com/huggingface/open_asr_leaderboard,https://huggingface.co/spaces/hf-audio/open_asr_leaderboard,,,Shayne,
Capabilities Evaluation,Text,LMSys Chatbot Arena,"A leaderboard of models based on Elo ratings where humans or models select their preferred response between two anonymous models. Chatbot Arena, MT-Bench, and 5-shot MMLU are used as benchmarks. This resource provides a general purpose, and GPT-4 biased perspective into model capabilities.",Frequently Updated,Hugging Face object,https://arxiv.org/abs/2306.05685,,https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md,https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard,,,Shayne,
License Selection,All,BigScience Open RAIL-M License,"Template for a responsible AI model license. Use restrictions include defamation, disinformation, and discrimination",8-2022,Webpage,,https://static1.squarespace.com/static/5c2a6d5c45776e85d1482a7e/t/6308bb4bba3a2a045b72a4b0/1661516619868/BigScience+Open+RAIL-M+License.pdf,,,,,Kevin,
License Selection,All,BigCode Open RAIL-M License,Template for a responsible AI model license. Use restrictions include generation and dissemination of malware,5-2023,Hugging Face object,,,,https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement,,,Kevin,
License Selection,All,Open RAIL-S License,"Template for a responsible AI source code license. Use restrictions relate to surveillance, synthetic media, healthcare and the criminal legal system",11-2023,Webpage,,https://www.licenses.ai/source-code-license,,,,,Kevin,
License Selection,All,AI Pubs Open RAIL-M License,"Template for a responsible AI model license where the model is intended for research use. Use restrictions relate to discrimination, transparency, and violating the law",3-2023,Webpage,,https://www.licenses.ai/ai-pubs-open-railm-vz1,,,,,Kevin,
License Selection,All,AI2 ImpACT-LR License,License for low risk AI artifacts (data and models) that allows for distribution of the artifact and its derivatives. Use restrictions include weapons development and military surveillance,7-2023,Webpage,,https://allenai.org/licenses/impact-lr,,,,,Kevin,
License Selection,All,AI2 ImpACT-MR License,License for medium risk AI artifacts (data and models) that does not allows for distribution of the artifact but does allow for distribution of its derivatives. Use restrictions include weapons development and military surveillance,7-2023,Webpage,,https://allenai.org/licenses/impact-mr,,,,,Kevin,
License Selection,All,Primer on AI2 ImpACT Licenses,A post by AI2 describing when and why an organization should use a specific ImpACT license,7-2023,Webpage,,https://allenai.org/impact-license,,,,,Kevin,
License Selection,All,Behavioral Use Licensing for Responsible AI,A paper that provides a theoretical framework for licenses inteded for open models with use restrictions,6-2022,Paper,https://dl.acm.org/doi/10.1145/3531146.3533143,,,,,,Kevin,
License Selection,All,"The Turning Way, Licensing",A gudie to reproducible research and licensing,Frequently Updated,Webpage,,https://the-turing-way.netlify.app/reproducible-research/licensing,,,,,Shayne,
License Selection,All,Legal Playbook For Natural Language Processing Researchers,"This playbook is a legal research resource for various activities related to data gathering, data governance, and disposition of an AI model available as a public resource.",2022,Webpage,,https://bigscience.huggingface.co/blog/legal-playbook-for-natural-language-processing-researchers,,,,,Shayne,
Model Training,Text,Training Compute-Optimal Language Models,"Proposes an optimal allocation of computational budget between model and dataset size, and shows experimental design for fitting scaling laws for compute allocation in a new setting.",3-2022,Paper,https://arxiv.org/abs/2203.15556,,,,,,Hailey,Effective Use of Resources section
Model Training,Text,Transformer Inference Arithmetic,A blog post on the inference costs of transformer-based LMs. Useful for providing more insight into deep learning accelerators and inference-relevant decisions to make when training a model.,3-2022,Webpage,,https://kipp.ly/transformer-inference-arithmetic/,,,,,Hailey,Educational Resources subsection
Model Training,Text,nanoGPT,"A minimal, stripped-down training codebase for teaching purposes and easily-hackable yet performant small-scale training.",12-2022,GitHub,,,https://github.com/karpathy/nanoGPT,,,,Hailey,Educational Resources subsection
Model Training,Text,Axolotl,"A repository for chat- or instruction-tuning language models, including through full fine-tuning, LoRA, QLoRA, and GPTQ.",4-2023,GitHub,,,https://github.com/OpenAccess-AI-Collective/axolotl,,,,Hailey,Common Repos subsection
Model Training,Text,Transformer Math 101,"An introductory blog post on training costs of LLMs, going over useful formulas and considerations from a high to low level",4-2023,Webpage,,https://blog.eleuther.ai/transformer-math/,,,,,Hailey,Educational Resources subsection
Model Training,Text,Scaling Data-Constrained Language Models,Demonstrates an optimal allocation of compute when dataset size is bounded,5-2023,Paper,https://arxiv.org/abs/2305.16264,,https://github.com/huggingface/datablations,https://huggingface.co/datablations,,,Hailey,Effective Use of Resources section
Model Training,Text,GPT-NeoX,A library for training large language models,Frequently Updated,GitHub,,,https://github.com/EleutherAI/gpt-neox,,,,Stella,
Model Training,Text,Megatron-DeepSpeed,A library for training large language models,Frequently Updated,GitHub,,,https://github.com/microsoft/Megatron-DeepSpeed,,,,Stella,
Model Training,Text+Vision,OpenCLIP,Supports training and inference for over 100 CLIP models,9-2021,GitHub,,,https://github.com/mlfoundations/open_clip,,,,Gabriel,
Model Training,Text+Vision,BLIP-2,Fine-tuned LLMs on multimodal data using a projection layer,1-2023,GitHub,https://arxiv.org/abs/2301.12597,,https://github.com/salesforce/LAVIS/tree/main/projects/blip2,,,,Gabriel,
Model Training,Text+Vision,OpenFlamingo,Open source implementation of Flamingo,3-2023,GitHub,https://arxiv.org/abs/2308.01390,https://laion.ai/blog/open-flamingo-v2/,https://github.com/mlfoundations/open_flamingo,https://huggingface.co/openflamingo,,,Gabriel,
Model Training,Text+Vision,LLaMA-Adapter,Fine-tuned LLMs on multimodal data using adapters,3-2023,GitHub,https://arxiv.org/abs/2304.15010,,https://github.com/OpenGVLab/LLaMA-Adapter,,,,Gabriel,
Model Training,Text+Vision,Otter,Multimodal models with Flamingo architecture,4-2023,GitHub,https://arxiv.org/abs/2311.04219,,https://github.com/Luodian/Otter,https://huggingface.co/spaces/Otter-AI/OtterHD-Demo,,,Gabriel,
Model Training,Text+Vision,MiniGPT4,Fine-tuned LLMs on multimodal data using a projection layer,4-2023,Webpage,https://arxiv.org/abs/2304.10592,https://minigpt-4.github.io/,https://github.com/Vision-CAIR/MiniGPT-4,https://huggingface.co/spaces/Vision-CAIR/minigpt4,,,Gabriel,
Model Training,Text+Vision,LLaVA,Fine-tuned LLMs on multimodal data using a projection layer,4-2023,Webpage,https://arxiv.org/abs/2310.03744,https://llava-vl.github.io/,https://github.com/haotian-liu/LLaVA,https://huggingface.co/spaces/badayvedat/LLaVA,,,Gabriel,
Model Training,Text+Vision,Kosmos-2,Multimodal models with CLIP backbones,6-2023,GitHub,https://arxiv.org/abs/2306.14824,,https://github.com/microsoft/unilm/tree/master/kosmos-2,https://huggingface.co/spaces/ydshieh/Kosmos-2,,,Gabriel,
Model Training,Vision,Pytorch Image Models (timm),"Hub for models, scripts and pre-trained weights for image classification",5-2019,GitHub,,,https://github.com/huggingface/pytorch-image-models,,,,Gabriel,
Model Training,Speech,Lhotse,Python library for handling speech data in machine learning projects,10-2023,GitHub,,https://github.com/lhotse-speech/lhotse,,,,,Nay,
Model Training,Text,QLoRa,An efficient finetuning approach that reduces memory usage while training.,5-2023,Paper,https://arxiv.org/abs/2305.14314,,https://github.com/artidoro/qlora,,,,Shayne,
Pretraining Data,Text,C4,"An English, cleaned version of Common Crawl's web crawl corpus (https://commoncrawl.org).",4-2019,Hugging Face object,https://arxiv.org/abs/1910.10683,https://commoncrawl.org,https://github.com/google-research/text-to-text-transfer-transformer#c4,https://huggingface.co/datasets/allenai/c4,https://aclanthology.org/2021.emnlp-main.98/,Data Study,Shayne,
Pretraining Data,Text,mC4,"The fully multilingual, cleaned version of Common Crawl's web crawl corpus (https://commoncrawl.org).",4-2019,Hugging Face object,https://arxiv.org/abs/1910.10683,https://commoncrawl.org,,https://huggingface.co/datasets/mc4,https://aclanthology.org/2022.tacl-1.4/,Data Study,Shayne,
Pretraining Data,Text,The Pile,"An 825GB English pretraining corpus that mixes portions of common crawl with 22 smaller, high-quality datasets combined together.",12-2020,Webpage,https://arxiv.org/abs/2101.00027,https://pile.eleuther.ai/,,,https://arxiv.org/abs/2201.07311,Datasheet,Shayne,
Pretraining Data,Text,ROOTS,"A massive multilingual pretraining corpus from BigScience, comprised of 1.6TB of text spanning 59 languages. It is a mix of OSCAR (https://oscar-project.org/) and the datasets found in the BigScience Catalogue (https://huggingface.co/spaces/bigscience/SourcingCatalog).",5-2022,Hugging Face object,https://arxiv.org/abs/2303.03915,https://bigscience.huggingface.co/,https://github.com/bigscience-workshop/bigscience/tree/master/data,https://huggingface.co/bigscience-data,https://arxiv.org/abs/2201.10066,Data Documentation,Shayne,
Pretraining Data,Text,Pile of Law,"An open-source, English dataset with ∼256GB of legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records.",11-2022,Hugging Face object,https://arxiv.org/abs/2207.00220,,,https://huggingface.co/datasets/pile-of-law/pile-of-law,,,Shayne,
Pretraining Data,Text,The Stack,"The Stack is a 6TB, permissively-licensed pretraining dataset from active GitHub repositories covering 358 programming languages.",11-2022,Hugging Face object,https://arxiv.org/abs/2211.15533,https://www.bigcode-project.org/docs/about/the-stack/#datasets-and-data-governance-tools-released-by-bigcode,https://github.com/bigcode-project/bigcode-dataset,https://huggingface.co/datasets/bigcode/the-stack,,,Shayne,
Pretraining Data,Text,peS2o,"A collection of ~40M creative open-access academic papers, cleaned, filtered, and formatted for pre-training of language models, originally derived from the Semantic Scholar Open Research Corpus (S2ORC).",1-2023,Hugging Face object,https://arxiv.org/abs/1911.02782,,,https://huggingface.co/datasets/allenai/peS2o,,,Shayne,
Pretraining Data,Text,The RefinedWeb,"An English-only,  web-only, deduplicated pretraining dataset of five trillion tokens.",6-2023,Hugging Face object,https://arxiv.org/abs/2306.01116,,,https://huggingface.co/datasets/tiiuae/falcon-refinedweb,,,Shayne,
Pretraining Data,Text,Dolma,"A pretraining dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.",8-2023,Hugging Face object,,,https://github.com/allenai/dolma,https://huggingface.co/datasets/allenai/dolma,https://github.com/allenai/dolma/blob/main/docs/assets/dolma-datasheet-v0.1.pdf,Datasheet,Shayne,
Pretraining Data,Text,OLC,"The Open License Corpus is a 228B token corpus of permissively-licensed, primarily English text data for pretraining.",8-2023,GitHub,https://arxiv.org/abs/2308.04430,,https://github.com/kernelmachine/silo-lm#download-data,https://huggingface.co/datasets/kernelmachine/open-license-corpus,,,Shayne,
Pretraining Data,Text,MADLAD-400,"A manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.",9-2023,Hugging Face object,https://arxiv.org/abs/2309.04662,,https://github.com/google-research/google-research/tree/master/madlad_400,https://huggingface.co/datasets/allenai/MADLAD-400,,,Shayne,
Pretraining Data,Text,CulturaX,"A pertaining dataset of 16T tokens, covering 167 languages, cleaned, deduplicated, and refined. Combines mC4 into 2020, with OSCAR project data up to 2023.",9-2023,Hugging Face object,https://arxiv.org/abs/2309.09400,,,https://huggingface.co/datasets/uonlp/CulturaX,,,Shayne,
Pretraining Data,Text,The Proof Pile 2,The Proof-Pile-2 is a 55 billion token dataset of mathematical and scientific documents.,9-2023,Hugging Face object,https://arxiv.org/abs/2310.10631,https://blog.eleuther.ai/llemma/,https://github.com/EleutherAI/math-lm,https://huggingface.co/datasets/EleutherAI/proof-pile-2,https://llemma-demo.github.io/,Explorer Tool,Shayne,
Pretraining Data,Text,RedPajama v2,"A pretraining dataset of 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.",10-2023,Hugging Face object,,https://www.together.ai/blog/redpajama-data-v2,https://github.com/togethercomputer/RedPajama-Data,https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2,,,Shayne,
Pretraining Data,Text,OpenWebMath,"A dataset containing the majority of the high-quality, mathematical text from the internet. It is filtered and extracted from over 200B HTML files on Common Crawl down to a set of 6.3 million documents containing a total of 14.7B tokens.",10-2023,Hugging Face object,https://arxiv.org/abs/2310.06786,,https://github.com/keirp/OpenWebMath,https://huggingface.co/datasets/open-web-math/open-web-math,,,Shayne,
Pretraining Data,Text,OPUS,The Open Parallel Corpus is a massive collection of translated text pairs from the web.,Frequently Updated,Webpage,,https://opus.nlpl.eu/,,,,,Shayne,
Pretraining Data,Text,OSCAR,The Open Super-large Crawled Aggregated coRpus provides web-based multilingual datasets across 166 languages.,Frequently Updated,Webpage,,https://oscar-project.org/,,,,,Shayne,
Pretraining Data,Text,WURA,"A manually audited multilingual pre-training corpus (document-level dataset) for 16 African languages and four  high-resource languages widely spoken in Africa (English, French, Arabic and Portuguese)",11-2023,Hugging Face object,https://aclanthology.org/2023.emnlp-main.11/,,,https://huggingface.co/datasets/castorini/wura,,,Shayne/David,
Pretraining Data,Text+Vision,WebVid-10M,10M videos with captions,4-2021,Webpage,https://arxiv.org/abs/2104.00650,https://maxbain.com/webvid-dataset/,https://github.com/m-bain/webvid,,,,Gabriel,
Pretraining Data,Text+Vision,DataComp-1B and CommonPool-13B,A large pool of 13B image-text pairs from CommonCrawl and a curated 1B subset,4-2023,Webpage,https://arxiv.org/abs/2304.14108,https://www.datacomp.ai/,https://github.com/mlfoundations/datacomp,https://huggingface.co/datasets/mlfoundations/datacomp_1b,,,Gabriel,
Pretraining Data,Text+Vision,MMC4,"Interleaved image-text data from Common Crawl (570M images, 43B tokens)",4-2023,GitHub,https://arxiv.org/abs/2304.06939,,https://github.com/allenai/mmc4,,https://huggingface.co/spaces/HuggingFaceM4/obelics_visualization,Dataset visualization,Gabriel,
Pretraining Data,Text+Vision,OBELICS,"Interleaved image-text data from Common Crawl (353 M images, 115B tokens)",6-2023,GitHub,https://arxiv.org/abs/2306.16527,https://huggingface.co/blog/idefics,https://github.com/huggingface/OBELICS,https://huggingface.co/datasets/HuggingFaceM4/OBELICS,,,Gabriel,
Pretraining Data,Text+Vision,LAION-5B,"A collection of over 5B image-text pairs collected from Common Crawl, optionally English-filtered",10-2023,Webpage,https://arxiv.org/abs/2210.08402,https://laion.ai/blog/laion-5b/,https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion5B.md,https://huggingface.co/datasets/laion/laion2B-en,https://rom1504.github.io/clip-retrieval/,Exploration tool,Gabriel,
Pretraining Data,Speech,LibriSpeech,960 hour read English speech from LibriVox audiobooks,2015,Webpage,http://www.danielpovey.com/files/2015_icassp_librispeech.pdf,https://www.openslr.org/12/,,,,,Nay,
Pretraining Data,Speech,The People’s Speech,30k hour conversational English dataset,11-2021,Hugging Face object,https://arxiv.org/abs/2111.09344,,,https://huggingface.co/datasets/MLCommons/peoples_speech,,,Nay,
Pretraining Data,Speech,GigaSpeech,40k hours (10k transcribed) multi-domain English speech corpus,2021,GitHub,https://arxiv.org/abs/2106.06909,,https://github.com/SpeechColab/GigaSpeech,,,,Nay,
Pretraining Data,Speech,Libri-Light,60k hour read English speech from LibriVox audiobooks,12-2019,GitHub,https://arxiv.org/abs/1912.07875,,https://github.com/facebookresearch/libri-light,,,,Nay,
Pretraining Data,Speech,Samrómur,"2,200 hour crowd-sourced corpus of Icelandic speech",2022,Webpage,,https://www.openslr.org/128/,,,,,Nay,
Pretraining Data,Speech,WenetSpeech,22.4k hour multi-domain corpus of Mandarin,10-2021,Webpage,https://arxiv.org/abs/2110.03370,https://www.openslr.org/121/,https://github.com/wenet-e2e/WenetSpeech,,,,Nay,
Pretraining Data,Speech,IndicSUPERB,"1,684 hour crowd-sourced corpus of 12 Indian languages",8-2022,Webpage,https://arxiv.org/abs/2208.11761,https://ai4bharat.iitm.ac.in/indicsuperb/,,,,,Nay,
Pretraining Data,Speech,Shrutilipi,"6,400 hour corpus of TV/Radio broadcasts from 12 Indian languages",8-2022,Webpage,https://arxiv.org/abs/2208.12666,https://ai4bharat.iitm.ac.in/shrutilipi/,,,,,Nay,
Pretraining Data,Speech,Common Voice,28k hours [as of 11/2023] of crowd-sourced read speech from 100+ languages,11-2023,Webpage,,https://commonvoice.mozilla.org/en/datasets,,,,,Nay,
Pretraining Data,Speech,VoxPopuli,400k hours of unlabelled speech from 23 languages of the European parliament,1-2021,GitHub,https://arxiv.org/abs/2101.00390,,https://github.com/facebookresearch/voxpopuli,,,,Nay,
Pretraining Data,Speech,Golos,"1,240 hours of crowd-sourced Russian speech",6-2021,Webpage,https://arxiv.org/abs/2106.10161,https://www.openslr.org/114/,https://github.com/sberdevices/golos,,,,Nay,
Risk & Harm Assessments,All,Taxonomy of Harms,Taxonomy of possible harms from generative AI systems to check your model against.,10-2023,Paper,,https://arxiv.org/abs/2310.11986,,,,,Laura & Maribeth,
Risk & Harm Assessments,All,Safety evaluation repository,"A repository of safety evaluations, across all modalities and harms, as of late 2023. Useful for delving deeper if the following evaluations don't meet your needs.",10-2023,Webpage,,https://dpmd.ai/46CPd58,,,,,Maribeth,
Risk & Harm Assessments,Speech,Racial disparities in automated speech recognition,A discussion of racial disparities and inclusiveness in automated speech recognition.,3-2020,Paper,,https://www.pnas.org/doi/10.1073/pnas.1915768117,,,,,Laura & Maribeth,
Risk & Harm Assessments,Speech,From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology,,5-2022,Paper,,https://aclanthology.org/2022.acl-long.385/ ,,,,,Laura & Maribeth,
Risk & Harm Assessments,Text,BBQ,Bias Benchmark for QA (BBQ) is a dataset consisting of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine different social dimensions relevant for U.S. English-speaking contexts.,10-2021,GitHub,https://arxiv.org/abs/2110.08193,,https://github.com/nyu-mll/BBQ,,,,Laura & Maribeth,
Risk & Harm Assessments,Text,RealToxicityPrompts,A dataset of 100k sentence snippets from the web for researchers to further address the risk of neural toxic degeneration in models.,9-2020,GitHub,https://arxiv.org/abs/2009.11462,https://toxicdegeneration.allenai.org/,https://github.com/allenai/real-toxicity-prompts,,,,Laura & Maribeth,
Risk & Harm Assessments,Text,FactualityPrompt,A benchmark to measure factuality in language models.,6-2022,GitHub,https://arxiv.org/abs/2206.04624,,https://github.com/nayeon7lee/FactualityPrompt,,,,Laura & Maribeth,
Risk & Harm Assessments,Text,Head-to-Tail,"For knowledge testing: a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. ",6-2023,Paper,https://arxiv.org/abs/2308.10168,https://paperswithcode.com/paper/head-to-tail-how-knowledgeable-are-large,,,,,Laura & Maribeth,
Risk & Harm Assessments,Text,Hallucinations,Public LLM leaderboard computed using Vectara's Hallucination Evaluation Model. This evaluates how often an LLM introduces hallucinations when summarizing a document. ,10-2023,GitHub,,https://github.com/vectara/hallucination-leaderboard,,https://huggingface.co/vectara/hallucination_evaluation_model,,,Laura & Maribeth,
Risk & Harm Assessments,Vision,StableBias,"Bias testing benchmark for Image to Text models, based on gender-occupation associations.",3-2023,Hugging Face object,,https://arxiv.org/abs/2303.11408,,https://huggingface.co/spaces/society-ethics/StableBias,,,Laura & Maribeth,
Risk & Harm Assessments,Text,Purple Llama CyberSecEval,"A benchmark for coding assistants, measuring their propensity to generate insecure code and level of compliance when asked to assist in cyberattacks.",12-2023,Webpage,,https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/,https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks,,,,Shayne,
Risk & Harm Assessments,Text,Purple Llama Guard,A tool to identify and protect against malicious inputs to LLMs.,12-2023,Webpage,https://arxiv.org/abs/2312.06674,https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/,https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard,,,,Shayne,
Risk & Harm Assessments,Text,HolisticBias,"A bias and toxicity benchmark using templated sentences, covering nearly 600 descriptor terms across 13 different demographic axes, for a total of 450k examples",10-2022,GitHub,https://arxiv.org/pdf/2205.09209.pdf,https://ai.meta.com/research/publications/im-sorry-to-hear-that-finding-new-biases-in-language-models-with-a-holistic-descriptor-dataset/,https://github.com/facebookresearch/ResponsibleNLP/tree/main/holistic_bias,,https://arxiv.org/pdf/2305.13198.pdf,"A new, multilingual version may be coming soon. Paper is out, dataset has not yet appeared",Maribeth,
Risk & Harm Assessments,Text,SimpleSafetyTests,"Small probe set (100 English text prompts) covering severe harms: child abuse, suicide, self-harm and eating disorders, scams and fraud, illegal items, and physical harm",11-2023,GitHub,https://arxiv.org/abs/2311.08370,,https://github.com/bertiev/SimpleSafetyTests,,,,Maribeth,
Risk & Harm Assessments,Vision,SneakyPrompt,Automated jailbreaking method to generate NSFW content even with models that have filters applied,5-2023,GitHub,https://arxiv.org/abs/2305.12082,,https://github.com/Yuchen413/text2image_safety,,,,Maribeth,
Risk & Harm Assessments,Text+Vision,Crossmodal-3600,Image captioning evaluation with geographically diverse images in 36 languages,5-2022,GitHub,https://arxiv.org/abs/2205.12522,,https://google.github.io/crossmodal-3600/,,,,Maribeth,
Usage Monitoring,All,Llama 2 Responsible Use Guide,Guidance for downstream developers on how to responsibly build with Llama 2. Includes details on how to report issues and instructions related to red-teaming and RLHF,12-2023,Webpage,,https://ai.meta.com/llama/responsible-use-guide/,,,,,Kevin,
Usage Monitoring,All,BigScience Ethical Charter,"Outlines BigScience's core values and how they promote them, which in turn guides use restrictions",,Webpage,,https://bigscience.huggingface.co/blog/bigscience-ethical-charter,,,,,Kevin,